{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "papermill": {
     "duration": 0.907452,
     "end_time": "2020-11-12T09:40:40.044736",
     "exception": false,
     "start_time": "2020-11-12T09:40:39.137284",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from iterstrat.ml_stratifiers import MultilabelStratifiedKFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
    "papermill": {
     "duration": 1.66405,
     "end_time": "2020-11-12T09:40:41.728012",
     "exception": false,
     "start_time": "2020-11-12T09:40:40.063962",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import copy\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import QuantileTransformer\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler, MinMaxScaler\n",
    "\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "papermill": {
     "duration": 0.046219,
     "end_time": "2020-11-12T09:40:41.803907",
     "exception": false,
     "start_time": "2020-11-12T09:40:41.757688",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def seed_everything(seed=1903):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    \n",
    "seed_everything(seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "papermill": {
     "duration": 6.614587,
     "end_time": "2020-11-12T09:40:48.448083",
     "exception": false,
     "start_time": "2020-11-12T09:40:41.833496",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_features = pd.read_csv('data/raw/train_features.csv')\n",
    "train_targets_scored = pd.read_csv('data/raw/train_targets_scored.csv')\n",
    "train_targets_nonscored = pd.read_csv('data/raw/train_targets_nonscored.csv')\n",
    "\n",
    "test_features = pd.read_csv('data/raw/test_features.csv')\n",
    "sample_submission = pd.read_csv('data/raw/sample_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "papermill": {
     "duration": 0.029154,
     "end_time": "2020-11-12T09:40:48.497291",
     "exception": false,
     "start_time": "2020-11-12T09:40:48.468137",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "GENES = [col for col in train_features.columns if col.startswith('g-')]\n",
    "CELLS = [col for col in train_features.columns if col.startswith('c-')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "papermill": {
     "duration": 0.393802,
     "end_time": "2020-11-12T09:40:48.909538",
     "exception": false,
     "start_time": "2020-11-12T09:40:48.515736",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(21948, 876)\n",
      "(3624, 876)\n",
      "(21948, 207)\n"
     ]
    }
   ],
   "source": [
    "train = train_features.merge(train_targets_scored, on='sig_id')\n",
    "train = train[train['cp_type']!='ctl_vehicle'].reset_index(drop=True)\n",
    "test = test_features[test_features['cp_type']!='ctl_vehicle'].reset_index(drop=True)\n",
    "\n",
    "target = train[train_targets_scored.columns]\n",
    "\n",
    "train = train[[col for col in train if col not in target.columns or col == 'sig_id']]\n",
    "\n",
    "print(train.shape)\n",
    "print(test.shape)\n",
    "print(target.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "papermill": {
     "duration": 0.038747,
     "end_time": "2020-11-12T09:40:48.967914",
     "exception": false,
     "start_time": "2020-11-12T09:40:48.929167",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "target_cols = target.drop('sig_id', axis=1).columns.values.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.019757,
     "end_time": "2020-11-12T09:40:49.175631",
     "exception": false,
     "start_time": "2020-11-12T09:40:49.155874",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# **CV Folds**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "papermill": {
     "duration": 2.729892,
     "end_time": "2020-11-12T09:40:51.925466",
     "exception": false,
     "start_time": "2020-11-12T09:40:49.195574",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "folds = train.copy()\n",
    "\n",
    "mskf = MultilabelStratifiedKFold(n_splits=7)\n",
    "\n",
    "for f, (t_idx, v_idx) in enumerate(mskf.split(X=train, y=target)):\n",
    "    folds.loc[v_idx, 'kfold'] = int(f)\n",
    "\n",
    "folds['kfold'] = folds['kfold'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "papermill": {
     "duration": 0.059554,
     "end_time": "2020-11-12T09:40:52.005477",
     "exception": false,
     "start_time": "2020-11-12T09:40:51.945923",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def process_data(data):\n",
    "    df = data.copy()\n",
    "    \n",
    "    features_g = GENES\n",
    "    features_c = CELLS\n",
    "    \n",
    "    df['g_sum'] = df[features_g].sum(axis = 1)\n",
    "    df['g_mean'] = df[features_g].mean(axis = 1)\n",
    "    df['g_std'] = df[features_g].std(axis = 1)\n",
    "    df['g_kurt'] = df[features_g].kurtosis(axis = 1)\n",
    "    df['g_skew'] = df[features_g].skew(axis = 1)\n",
    "    df['c_sum'] = df[features_c].sum(axis = 1)\n",
    "    df['c_mean'] = df[features_c].mean(axis = 1)\n",
    "    df['c_std'] = df[features_c].std(axis = 1)\n",
    "    df['c_kurt'] = df[features_c].kurtosis(axis = 1)\n",
    "    df['c_skew'] = df[features_c].skew(axis = 1)\n",
    "    df['gc_sum'] = df[features_g + features_c].sum(axis = 1)\n",
    "    df['gc_mean'] = df[features_g + features_c].mean(axis = 1)\n",
    "    df['gc_std'] = df[features_g + features_c].std(axis = 1)\n",
    "    df['gc_kurt'] = df[features_g + features_c].kurtosis(axis = 1)\n",
    "    df['gc_skew'] = df[features_g + features_c].skew(axis = 1)\n",
    "    \n",
    "    for feature in features_c:\n",
    "        df[f'{feature}_squared'] = df[feature] ** 2\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "def add_pca(train_df, valid_df, test_):\n",
    "    \n",
    "    # GENES\n",
    "    n_comp = 28\n",
    "\n",
    "    pca = PCA(n_components=n_comp, random_state=1903)\n",
    "    pipe = Pipeline([('scal', RobustScaler()), ('pca', pca)])\n",
    "    train2 = pipe.fit_transform(train_df[GENES])\n",
    "    valid2 = pipe.transform(valid_df[GENES])\n",
    "    test2 = pipe.transform(test_[GENES])\n",
    "\n",
    "    train2 = pd.DataFrame(train2, columns=[f'pca_G-{i}' for i in range(n_comp)])\n",
    "    valid2 = pd.DataFrame(valid2, columns=[f'pca_G-{i}' for i in range(n_comp)])\n",
    "    test2 = pd.DataFrame(test2, columns=[f'pca_G-{i}' for i in range(n_comp)])\n",
    "\n",
    "    train_df = pd.concat((train_df, train2), axis=1)\n",
    "    valid_df = pd.concat((valid_df, valid2), axis=1)\n",
    "    test_ = pd.concat((test_, test2), axis=1)\n",
    "\n",
    "    #CELLS\n",
    "    n_comp = 5\n",
    "\n",
    "    pca = PCA(n_components=n_comp, random_state=1903)\n",
    "    pipe = Pipeline([('scal', RobustScaler()), ('pca', pca)])\n",
    "    train2 = pipe.fit_transform(train_df[CELLS])\n",
    "    valid2 = pipe.transform(valid_df[CELLS])\n",
    "    test2 = pipe.transform(test_[CELLS])\n",
    "\n",
    "    train2 = pd.DataFrame(train2, columns=[f'pca_C-{i}' for i in range(n_comp)])\n",
    "    valid2 = pd.DataFrame(valid2, columns=[f'pca_C-{i}' for i in range(n_comp)])\n",
    "    test2 = pd.DataFrame(test2, columns=[f'pca_C-{i}' for i in range(n_comp)])\n",
    "\n",
    "    train_df = pd.concat((train_df, train2), axis=1)\n",
    "    valid_df = pd.concat((valid_df, valid2), axis=1)\n",
    "    test_ = pd.concat((test_, test2), axis=1)\n",
    "    \n",
    "    return train_df, valid_df, test_\n",
    "\n",
    "\n",
    "def var_tr(train_df, valid_df, test_):\n",
    "    \n",
    "    var_thresh = VarianceThreshold(threshold=0.9)\n",
    "\n",
    "    train_transformed = var_thresh.fit_transform(train_df.iloc[:, 4:])\n",
    "    valid_transformed = var_thresh.transform(valid_df.iloc[:, 4:])\n",
    "    test_transformed = var_thresh.transform(test_.iloc[:, 4:])\n",
    "\n",
    "    train_features = pd.DataFrame(train_df[['sig_id','cp_type','cp_time','cp_dose']].values.reshape(-1, 4),\\\n",
    "                                  columns=['sig_id','cp_type','cp_time','cp_dose'])\n",
    "    train_features = pd.concat([train_features, pd.DataFrame(train_transformed)], axis=1)\n",
    "    \n",
    "    valid_features = pd.DataFrame(valid_df[['sig_id','cp_type','cp_time','cp_dose']].values.reshape(-1, 4),\\\n",
    "                                  columns=['sig_id','cp_type','cp_time','cp_dose'])\n",
    "    valid_features = pd.concat([valid_features, pd.DataFrame(valid_transformed)], axis=1)\n",
    "\n",
    "    test_features = pd.DataFrame(test_[['sig_id','cp_type','cp_time','cp_dose']].values.reshape(-1, 4),\\\n",
    "                                 columns=['sig_id','cp_type','cp_time','cp_dose'])\n",
    "    test_features = pd.concat([test_features, pd.DataFrame(test_transformed)], axis=1)\n",
    "    \n",
    "    return train_features, valid_features, test_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.02003,
     "end_time": "2020-11-12T09:40:52.045518",
     "exception": false,
     "start_time": "2020-11-12T09:40:52.025488",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# **Utilities**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "papermill": {
     "duration": 0.041899,
     "end_time": "2020-11-12T09:40:52.107831",
     "exception": false,
     "start_time": "2020-11-12T09:40:52.065932",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class DfScaler(BaseEstimator, TransformerMixin):\n",
    "    '''\n",
    "    Wrapper of several sklearn scalers that keeps the dataframe structure\n",
    "    '''\n",
    "    def __init__(self, method='standard', feature_range=(0,1)):\n",
    "        super().__init__()\n",
    "        self.method = method\n",
    "        self._validate_input()\n",
    "        self.scale_ = None\n",
    "        if self.method == 'standard':\n",
    "            self.scl = StandardScaler()\n",
    "            self.mean_ = None\n",
    "        elif method == 'robust':\n",
    "            self.scl = RobustScaler()\n",
    "            self.center_ = None\n",
    "        elif method == 'minmax':\n",
    "            self.feature_range = feature_range\n",
    "            self.scl = MinMaxScaler(feature_range=self.feature_range)\n",
    "            self.min_ = None\n",
    "            self.data_min_ = None\n",
    "            self.data_max_ = None\n",
    "            self.data_range_ = None\n",
    "            self.n_samples_seen_ = None\n",
    "\n",
    "            \n",
    "    def _validate_input(self):\n",
    "        allowed_methods = [\"standard\", 'robust', 'minmax']\n",
    "        if self.method not in allowed_methods:\n",
    "            raise ValueError(f\"Can only use these methods: {allowed_methods} got method={self.method}\")\n",
    "    \n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        self.scl.fit(X)\n",
    "        if self.method == 'standard':\n",
    "            self.mean_ = pd.Series(self.scl.mean_, index=X.columns)\n",
    "        elif self.method == 'robust':\n",
    "            self.center_ = pd.Series(self.scl.center_, index=X.columns)\n",
    "        elif self.method == 'minmax':\n",
    "            self.min_ = pd.Series(self.scl.min_, index=X.columns)\n",
    "            self.data_min_ = pd.Series(self.scl.data_min_, index=X.columns)\n",
    "            self.data_max_ = pd.Series(self.scl.data_max_, index=X.columns)\n",
    "            self.data_range_ = self.data_max_ - self.data_min_\n",
    "            self.n_samples_seen_ = X.shape[0]\n",
    "        self.scale_ = pd.Series(self.scl.scale_, index=X.columns)\n",
    "        return self\n",
    "    \n",
    "    \n",
    "    def transform(self, X, y=None):\n",
    "        # assumes X is a DataFrame\n",
    "        Xscl = self.scl.transform(X)\n",
    "        Xscaled = pd.DataFrame(Xscl, index=X.index, columns=X.columns)\n",
    "        return Xscaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "papermill": {
     "duration": 0.031844,
     "end_time": "2020-11-12T09:40:52.159854",
     "exception": false,
     "start_time": "2020-11-12T09:40:52.128010",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def scale_data(train, valid, test):\n",
    "    \n",
    "    scl = DfScaler(method='robust')\n",
    "    scaled_train = scl.fit_transform(train[[col for col in train if col!='sig_id']])\n",
    "    scaled_train = pd.concat([train[['sig_id']], scaled_train], axis=1)\n",
    "    \n",
    "    scaled_valid = scl.transform(valid[[col for col in valid if col!='sig_id']])\n",
    "    scaled_valid = pd.concat([valid[['sig_id']], scaled_valid], axis=1)\n",
    "    \n",
    "    scaled_test = scl.transform(test[[col for col in test if col!='sig_id']])\n",
    "    scaled_test = pd.concat([test[['sig_id']], scaled_test], axis=1)\n",
    "    \n",
    "    return scaled_train, scaled_valid, scaled_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "papermill": {
     "duration": 0.033084,
     "end_time": "2020-11-12T09:40:52.213441",
     "exception": false,
     "start_time": "2020-11-12T09:40:52.180357",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class MoADataset:\n",
    "    def __init__(self, features, targets):\n",
    "        self.features = features\n",
    "        self.targets = targets\n",
    "        \n",
    "    def __len__(self):\n",
    "        return (self.features.shape[0])\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        dct = {\n",
    "            'x' : torch.tensor(self.features[idx, :], dtype=torch.float),\n",
    "            'y' : torch.tensor(self.targets[idx, :], dtype=torch.float)            \n",
    "        }\n",
    "        return dct\n",
    "    \n",
    "class TestDataset:\n",
    "    def __init__(self, features):\n",
    "        self.features = features\n",
    "        \n",
    "    def __len__(self):\n",
    "        return (self.features.shape[0])\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        dct = {\n",
    "            'x' : torch.tensor(self.features[idx, :], dtype=torch.float)\n",
    "        }\n",
    "        return dct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "papermill": {
     "duration": 0.037579,
     "end_time": "2020-11-12T09:40:52.271616",
     "exception": false,
     "start_time": "2020-11-12T09:40:52.234037",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train_fn(model, optimizer, scheduler, loss_fn, dataloader, device):\n",
    "    model.train()\n",
    "    final_loss = 0\n",
    "    \n",
    "    for data in dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        inputs, targets = data['x'].to(device), data['y'].to(device)\n",
    "        #print(inputs.shape)\n",
    "        outputs = model(inputs)\n",
    "        loss = loss_fn(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        \n",
    "        final_loss += loss.item()\n",
    "        \n",
    "    final_loss /= len(dataloader)\n",
    "    \n",
    "    return final_loss\n",
    "\n",
    "\n",
    "def valid_fn(model, loss_fn, dataloader, device):\n",
    "    model.eval()\n",
    "    final_loss = 0\n",
    "    valid_preds = []\n",
    "    \n",
    "    for data in dataloader:\n",
    "        inputs, targets = data['x'].to(device), data['y'].to(device)\n",
    "        outputs = model(inputs)\n",
    "        loss = loss_fn(outputs, targets)\n",
    "        \n",
    "        final_loss += loss.item()\n",
    "        valid_preds.append(outputs.sigmoid().detach().cpu().numpy())\n",
    "        \n",
    "    final_loss /= len(dataloader)\n",
    "    valid_preds = np.concatenate(valid_preds)\n",
    "    \n",
    "    return final_loss, valid_preds\n",
    "\n",
    "\n",
    "def inference_fn(model, dataloader, device):\n",
    "    model.eval()\n",
    "    preds = []\n",
    "    \n",
    "    for data in dataloader:\n",
    "        inputs = data['x'].to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(inputs)\n",
    "        \n",
    "        preds.append(outputs.sigmoid().detach().cpu().numpy())\n",
    "        \n",
    "    preds = np.concatenate(preds)\n",
    "    \n",
    "    return preds\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "papermill": {
     "duration": 0.045362,
     "end_time": "2020-11-12T09:40:52.337280",
     "exception": false,
     "start_time": "2020-11-12T09:40:52.291918",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, num_features, num_targets, hidden_size):\n",
    "        super(Model, self).__init__()\n",
    "        self.batch_norm1 = nn.BatchNorm1d(num_features)\n",
    "        self.dropout1 = nn.Dropout(0.25)\n",
    "        self.dense1 = nn.utils.weight_norm(nn.Linear(num_features, hidden_size))\n",
    "        \n",
    "        self.batch_norm2 = nn.BatchNorm1d(hidden_size)\n",
    "        self.dropout2 = nn.Dropout(0.25)\n",
    "        self.dense2 = nn.utils.weight_norm(nn.Linear(hidden_size, hidden_size))\n",
    "        \n",
    "        self.batch_norm3 = nn.BatchNorm1d(hidden_size)\n",
    "        self.dropout3 = nn.Dropout(0.25)\n",
    "        self.dense3 = nn.utils.weight_norm(nn.Linear(hidden_size, num_targets))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.batch_norm1(x)\n",
    "        x = self.dropout1(x)\n",
    "        x = F.leaky_relu(self.dense1(x), 1e-3)\n",
    "        \n",
    "        x = self.batch_norm2(x)\n",
    "        x = self.dropout2(x)\n",
    "        x = F.relu(self.dense2(x))\n",
    "        \n",
    "        x = self.batch_norm3(x)\n",
    "        x = self.dropout3(x)\n",
    "        x = self.dense3(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "papermill": {
     "duration": 0.397904,
     "end_time": "2020-11-12T09:40:52.755541",
     "exception": false,
     "start_time": "2020-11-12T09:40:52.357637",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# HyperParameters\n",
    "\n",
    "DEVICE = ('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "EPOCHS = 40\n",
    "BATCH_SIZE = 256\n",
    "LEARNING_RATE = 3e-4\n",
    "WEIGHT_DECAY = 1e-5\n",
    "NFOLDS = 7\n",
    "EARLY_STOPPING_STEPS = 5\n",
    "EARLY_STOP = True\n",
    "\n",
    "hidden_size=2048"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "papermill": {
     "duration": 0.056001,
     "end_time": "2020-11-12T09:40:52.832205",
     "exception": false,
     "start_time": "2020-11-12T09:40:52.776204",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def run_training(train, test, fold, seed):\n",
    "    \n",
    "    test_ = test.copy()\n",
    "    \n",
    "    seed_everything(seed)\n",
    "    \n",
    "    trn_idx = train[train['kfold'] != fold].index\n",
    "    val_idx = train[train['kfold'] == fold].index\n",
    "    \n",
    "    train_df = train[train['kfold'] != fold].reset_index(drop=True)\n",
    "    valid_df = train[train['kfold'] == fold].reset_index(drop=True)\n",
    "    \n",
    "    del train_df['kfold']\n",
    "    del valid_df['kfold']\n",
    "    \n",
    "    train_df, valid_df, test_ = add_pca(train_df, valid_df, test_)\n",
    "    \n",
    "    train_df = process_data(train_df)\n",
    "    valid_df = process_data(valid_df)\n",
    "    test_ = process_data(test_)\n",
    "    \n",
    "    train_df, valid_df, test_ = var_tr(train_df, valid_df, test_)\n",
    "    \n",
    "    train_df = train_df.drop('cp_type', axis=1)\n",
    "    valid_df = valid_df.drop('cp_type', axis=1)\n",
    "    test_ = test_.drop('cp_type', axis=1)\n",
    "    \n",
    "    train_df['time_dose'] = train_df['cp_time'].astype(str)+train_df['cp_dose']\n",
    "    valid_df['time_dose'] = valid_df['cp_time'].astype(str)+valid_df['cp_dose']\n",
    "    test_['time_dose'] = test_['cp_time'].astype(str)+test_['cp_dose']\n",
    "    \n",
    "    train_df = pd.get_dummies(train_df, columns=['cp_time','cp_dose','time_dose'])\n",
    "    valid_df = pd.get_dummies(valid_df, columns=['cp_time','cp_dose','time_dose'])\n",
    "    test_ = pd.get_dummies(test_, columns=['cp_time','cp_dose','time_dose'])\n",
    "    \n",
    "    feature_cols = [c for c in train_df.columns if c not in target_cols]\n",
    "    feature_cols = [c for c in feature_cols if c not in ['kfold','sig_id']]\n",
    "    num_features=len(feature_cols)\n",
    "    num_targets=len(target_cols)\n",
    "    \n",
    "    #scaling\n",
    "    train_df, valid_df, test_ = scale_data(train_df, valid_df, test_)\n",
    "    \n",
    "    x_train, y_train  = train_df[feature_cols].values, target.iloc[trn_idx, :][target_cols].values\n",
    "    x_valid, y_valid =  valid_df[feature_cols].values, target.iloc[val_idx, :][target_cols].values\n",
    "    \n",
    "    train_dataset = MoADataset(x_train, y_train)\n",
    "    valid_dataset = MoADataset(x_valid, y_valid)\n",
    "    trainloader = torch.utils.data.DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "    validloader = torch.utils.data.DataLoader(valid_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "    model = Model(\n",
    "        num_features=num_features,\n",
    "        num_targets=num_targets,\n",
    "        hidden_size=hidden_size,\n",
    "    )\n",
    "    \n",
    "    model.to(DEVICE)\n",
    "    \n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
    "    scheduler = optim.lr_scheduler.OneCycleLR(optimizer=optimizer, pct_start=0.1, div_factor=1e3, \n",
    "                                              max_lr=1e-2, epochs=EPOCHS, steps_per_epoch=len(trainloader))\n",
    "    \n",
    "    loss_fn = nn.BCEWithLogitsLoss()\n",
    "    \n",
    "    early_stopping_steps = EARLY_STOPPING_STEPS\n",
    "    early_step = 0\n",
    "    \n",
    "    oof = np.zeros((len(train), target.iloc[:, 1:].shape[1]))\n",
    "    best_loss = np.inf\n",
    "    \n",
    "    print(f\"FOLD: {fold}, n_features={num_features}\")\n",
    "    \n",
    "    for epoch in range(EPOCHS):\n",
    "        \n",
    "        train_loss = train_fn(model, optimizer,scheduler, loss_fn, trainloader, DEVICE)\n",
    "        valid_loss, valid_preds = valid_fn(model, loss_fn, validloader, DEVICE)\n",
    "        print(f\"EPOCH: {epoch}, train_loss: {train_loss}, valid_loss: {valid_loss}\")\n",
    "        \n",
    "        if valid_loss < best_loss:\n",
    "            \n",
    "            best_loss = valid_loss\n",
    "            oof[val_idx] = valid_preds\n",
    "            torch.save(model.state_dict(), f\"FOLD{fold}_.pth\")\n",
    "            early_step = 0\n",
    "        \n",
    "        else:\n",
    "            if EARLY_STOP:\n",
    "                early_step += 1\n",
    "                if (early_step >= early_stopping_steps):\n",
    "                    break\n",
    "            \n",
    "    \n",
    "    #--------------------- PREDICTION---------------------\n",
    "    x_test = test_[feature_cols].values\n",
    "    testdataset = TestDataset(x_test)\n",
    "    testloader = torch.utils.data.DataLoader(testdataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "    \n",
    "    model = Model(\n",
    "        num_features=num_features,\n",
    "        num_targets=num_targets,\n",
    "        hidden_size=hidden_size,\n",
    "    )\n",
    "    \n",
    "    model.load_state_dict(torch.load(f\"FOLD{fold}_.pth\"))\n",
    "    model.to(DEVICE)\n",
    "    \n",
    "    predictions = np.zeros((len(test_), target.iloc[:, 1:].shape[1]))\n",
    "    predictions = inference_fn(model, testloader, DEVICE)\n",
    "    \n",
    "    return oof, predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "papermill": {
     "duration": 0.030538,
     "end_time": "2020-11-12T09:40:52.883179",
     "exception": false,
     "start_time": "2020-11-12T09:40:52.852641",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def run_k_fold(NFOLDS, seed):\n",
    "    oof = np.zeros((len(train), len(target_cols)))\n",
    "    predictions = np.zeros((len(test), len(target_cols)))\n",
    "    \n",
    "    for fold in range(NFOLDS):\n",
    "        oof_, pred_ = run_training(folds, test, fold, seed)\n",
    "        \n",
    "        predictions += pred_ / NFOLDS\n",
    "        oof += oof_\n",
    "        \n",
    "    return oof, predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 759.617682,
     "end_time": "2020-11-12T09:53:32.521602",
     "exception": false,
     "start_time": "2020-11-12T09:40:52.903920",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SEED: 1903\n",
      "FOLD: 0, n_features=906\n",
      "EPOCH: 0, train_loss: 0.6891456222211992, valid_loss: 0.4691091019373674\n",
      "EPOCH: 1, train_loss: 0.10199048214063451, valid_loss: 0.0206038963336211\n",
      "EPOCH: 2, train_loss: 0.01986665532899064, valid_loss: 0.02000745987662902\n",
      "EPOCH: 3, train_loss: 0.018495037421785498, valid_loss: 0.018139184094392337\n",
      "EPOCH: 4, train_loss: 0.017817321082426084, valid_loss: 0.01784716138186363\n",
      "EPOCH: 5, train_loss: 0.017339889551638753, valid_loss: 0.017402811262470026\n",
      "EPOCH: 6, train_loss: 0.01710958017439053, valid_loss: 0.017176977573679045\n",
      "EPOCH: 7, train_loss: 0.016944393062511005, valid_loss: 0.017100217537238047\n",
      "EPOCH: 8, train_loss: 0.016995989063100236, valid_loss: 0.017052864727492515\n",
      "EPOCH: 9, train_loss: 0.016964867207649593, valid_loss: 0.017271751108077858\n",
      "EPOCH: 10, train_loss: 0.017023750228454936, valid_loss: 0.0169291110136188\n",
      "EPOCH: 11, train_loss: 0.017008958925568574, valid_loss: 0.016999345201139267\n",
      "EPOCH: 12, train_loss: 0.016989293822867645, valid_loss: 0.01693201745645358\n",
      "EPOCH: 13, train_loss: 0.016960201810139255, valid_loss: 0.017120108724786684\n",
      "EPOCH: 14, train_loss: 0.01696347099149952, valid_loss: 0.01719747927899544\n",
      "EPOCH: 15, train_loss: 0.01697383958191888, valid_loss: 0.017015128539731868\n",
      "FOLD: 1, n_features=904\n",
      "EPOCH: 0, train_loss: 0.6896568444129583, valid_loss: 0.4610589857284839\n",
      "EPOCH: 1, train_loss: 0.10246784136806791, valid_loss: 0.020864334243994493\n",
      "EPOCH: 2, train_loss: 0.019943204658055626, valid_loss: 0.019740772505219165\n",
      "EPOCH: 3, train_loss: 0.018528183645291907, valid_loss: 0.018378913545837768\n",
      "EPOCH: 4, train_loss: 0.017908973984319617, valid_loss: 0.017436791354647048\n",
      "EPOCH: 5, train_loss: 0.017373769455966918, valid_loss: 0.01737548869389754\n",
      "EPOCH: 6, train_loss: 0.01715339332617618, valid_loss: 0.017326960196861856\n",
      "EPOCH: 7, train_loss: 0.017011764471897402, valid_loss: 0.01695719762490346\n",
      "EPOCH: 8, train_loss: 0.016996324741961184, valid_loss: 0.01694484642491891\n",
      "EPOCH: 9, train_loss: 0.01704117952770478, valid_loss: 0.01681086650261512\n",
      "EPOCH: 10, train_loss: 0.017001784394016943, valid_loss: 0.01696201005520729\n",
      "EPOCH: 11, train_loss: 0.017077757448360726, valid_loss: 0.01700023404107644\n",
      "EPOCH: 12, train_loss: 0.017104637733585126, valid_loss: 0.016982309663524993\n",
      "EPOCH: 13, train_loss: 0.01710597238176175, valid_loss: 0.016975627567332525\n",
      "EPOCH: 14, train_loss: 0.017033130043765176, valid_loss: 0.016838263147152387\n",
      "FOLD: 2, n_features=905\n",
      "EPOCH: 0, train_loss: 0.6905995353653624, valid_loss: 0.507314356473776\n",
      "EPOCH: 1, train_loss: 0.10367945156286697, valid_loss: 0.02074141399218486\n",
      "EPOCH: 2, train_loss: 0.019684911504186487, valid_loss: 0.019517366989300802\n",
      "EPOCH: 3, train_loss: 0.0184923396823374, valid_loss: 0.018802789541391227\n",
      "EPOCH: 4, train_loss: 0.017952499067964586, valid_loss: 0.017359174573077604\n",
      "EPOCH: 5, train_loss: 0.017280044210319583, valid_loss: 0.017272184101434853\n",
      "EPOCH: 6, train_loss: 0.016994499337129498, valid_loss: 0.01699672954586836\n",
      "EPOCH: 7, train_loss: 0.016924730816705, valid_loss: 0.016937321051955223\n",
      "EPOCH: 8, train_loss: 0.016947025861087685, valid_loss: 0.01687324333649415\n",
      "EPOCH: 9, train_loss: 0.016887169029261614, valid_loss: 0.016907088745098848\n",
      "EPOCH: 10, train_loss: 0.01689117931685335, valid_loss: 0.01688193021198878\n",
      "EPOCH: 11, train_loss: 0.01701107368225584, valid_loss: 0.016914724802168515\n",
      "EPOCH: 12, train_loss: 0.017022360191756004, valid_loss: 0.016965615992935803\n",
      "EPOCH: 13, train_loss: 0.016993510989925346, valid_loss: 0.017086849762843206\n",
      "FOLD: 3, n_features=903\n",
      "EPOCH: 0, train_loss: 0.6890630315284472, valid_loss: 0.4601048918870779\n",
      "EPOCH: 1, train_loss: 0.1025844604985134, valid_loss: 0.02068767581994717\n",
      "EPOCH: 2, train_loss: 0.019881079678197165, valid_loss: 0.01932047737332491\n",
      "EPOCH: 3, train_loss: 0.018315260550258932, valid_loss: 0.018371618281190213\n",
      "EPOCH: 4, train_loss: 0.017839549046412512, valid_loss: 0.017677623205460034\n",
      "EPOCH: 5, train_loss: 0.017317173033449297, valid_loss: 0.0177931202432284\n",
      "EPOCH: 6, train_loss: 0.017097925792473392, valid_loss: 0.017149153237159435\n",
      "EPOCH: 7, train_loss: 0.0169084624362153, valid_loss: 0.017216756939888\n",
      "EPOCH: 8, train_loss: 0.016897784848068212, valid_loss: 0.01710621711726372\n",
      "EPOCH: 9, train_loss: 0.01689983585950088, valid_loss: 0.017256054955606278\n",
      "EPOCH: 10, train_loss: 0.016935889206423953, valid_loss: 0.017184198977282412\n",
      "EPOCH: 11, train_loss: 0.016963687385558278, valid_loss: 0.017004711410174005\n",
      "EPOCH: 12, train_loss: 0.01696462249635039, valid_loss: 0.017192369040388327\n",
      "EPOCH: 13, train_loss: 0.01687846561843479, valid_loss: 0.01720093720807479\n",
      "EPOCH: 14, train_loss: 0.01695936774784649, valid_loss: 0.017073831520974636\n",
      "EPOCH: 15, train_loss: 0.016923565014794067, valid_loss: 0.01698500312005098\n",
      "EPOCH: 16, train_loss: 0.01691287723244042, valid_loss: 0.017242116280473195\n",
      "EPOCH: 17, train_loss: 0.01689839077408652, valid_loss: 0.01754729815114003\n",
      "EPOCH: 18, train_loss: 0.016867619158851134, valid_loss: 0.016982560189297564\n",
      "EPOCH: 19, train_loss: 0.01681948235459827, valid_loss: 0.01682822578228437\n",
      "EPOCH: 20, train_loss: 0.016695395854578632, valid_loss: 0.016864988379753552\n",
      "EPOCH: 21, train_loss: 0.016631520265159575, valid_loss: 0.0167215562497194\n",
      "EPOCH: 22, train_loss: 0.016580811814983953, valid_loss: 0.01672542653977871\n",
      "EPOCH: 23, train_loss: 0.01652645755156472, valid_loss: 0.01669725152448966\n",
      "EPOCH: 24, train_loss: 0.016456414983179922, valid_loss: 0.01666064352656786\n",
      "EPOCH: 25, train_loss: 0.01632148157050078, valid_loss: 0.016566580089812096\n",
      "EPOCH: 26, train_loss: 0.016180206455189635, valid_loss: 0.01644187943580059\n"
     ]
    }
   ],
   "source": [
    "# Averaging on multiple SEEDS\n",
    "\n",
    "SEED = [1903, 1881] # , 346, 4578, 21\n",
    "oof = np.zeros((len(train), len(target_cols)))\n",
    "predictions = np.zeros((len(test), len(target_cols)))\n",
    "\n",
    "for seed in SEED:\n",
    "    print(f\"SEED: {seed}\")\n",
    "    \n",
    "    oof_, predictions_ = run_k_fold(NFOLDS, seed)\n",
    "    oof += oof_ / len(SEED)\n",
    "    predictions += predictions_ / len(SEED)\n",
    "\n",
    "train[target_cols] = oof\n",
    "test[target_cols] = predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "papermill": {
     "duration": 1.17296,
     "end_time": "2020-11-12T09:53:33.861903",
     "exception": false,
     "start_time": "2020-11-12T09:53:32.688943",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV log_loss:  0.015347327538593266\n"
     ]
    }
   ],
   "source": [
    "valid_results = train_targets_scored.drop(columns=target_cols).merge(train[['sig_id']+target_cols], on='sig_id', how='left').fillna(0)\n",
    "\n",
    "y_true = train_targets_scored[target_cols].values\n",
    "y_pred = valid_results[target_cols].values\n",
    "\n",
    "score = 0\n",
    "for i in range(len(target_cols)):\n",
    "    score_ = log_loss(y_true[:, i], y_pred[:, i])\n",
    "    score += score_ / target.shape[1]\n",
    "    \n",
    "print(\"CV log_loss: \", score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-12T09:53:34.362377Z",
     "iopub.status.busy": "2020-11-12T09:53:34.360757Z",
     "iopub.status.idle": "2020-11-12T09:53:36.869459Z",
     "shell.execute_reply": "2020-11-12T09:53:36.868352Z"
    },
    "papermill": {
     "duration": 2.842077,
     "end_time": "2020-11-12T09:53:36.869588",
     "exception": false,
     "start_time": "2020-11-12T09:53:34.027511",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "sub = sample_submission.drop(columns=target_cols).merge(test[['sig_id']+target_cols], on='sig_id', how='left').fillna(0)\n",
    "sub.to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.166775,
     "end_time": "2020-11-12T09:53:37.204833",
     "exception": false,
     "start_time": "2020-11-12T09:53:37.038058",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "papermill": {
   "duration": 782.985527,
   "end_time": "2020-11-12T09:53:37.976116",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2020-11-12T09:40:34.990589",
   "version": "2.1.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
